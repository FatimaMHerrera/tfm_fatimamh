{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFM : Pruebas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [url_ayuda_secuencias](https://med.stanford.edu/content/dam/sm/genetics/documents/gene211/schedule/Lecture4_Sequence_Comparison-2014.pdf)\n",
    "- [url_ayuda_para_npx](https://olink.com/faq/what-is-npx/)\n",
    "- [url_ayuda_proteins_existence](https://www.uniprot.org/help/protein_existence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To interact with the operative system.\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "os.chdir('..')\n",
    "\n",
    "# For time and dates.\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# For the use of warnings.\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=(FutureWarning, UserWarning))\n",
    "\n",
    "# To use DataFrames.\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# To treat mathematical objects.\n",
    "import numpy as np \n",
    "\n",
    "# To get online information.\n",
    "import requests\n",
    "\n",
    "# To get random samples or numbers.\n",
    "import random as rnd\n",
    "\n",
    "# For the typing hints.\n",
    "from typing import Dict, List\n",
    "\n",
    "# To get information about the errors.\n",
    "import traceback\n",
    "\n",
    "# To the graphics.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# To train, preprocess and evaluate models.\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import (train_test_split, KFold, learning_curve)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import (GradientBoostingRegressor, RandomForestRegressor)\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Para optimizar hiperparámetros.\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_param_importances\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_contour\n",
    "\n",
    "# Para guardar y cargar modelos.\n",
    "import joblib\n",
    "\n",
    "# Para guardar y cargar parámetros.\n",
    "import json\n",
    "\n",
    "# Personal imports.\n",
    "from utils import DataLoader, DataFrameOptimizer, reduce_mem_usage\n",
    "from bioinfo import UnitProtInfo\n",
    "from fe import FeatureEngineeringNew, full_FeatureEngineeringNew\n",
    "from eda import EdaNew, full_EdaNew\n",
    "from modelling import DataPreparationToModelNew, Metricas\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[3636, 5742, 7117, 26210, 41883]\n"
     ]
    }
   ],
   "source": [
    "def sample_dataframes(dataframes, seed=42, sample_size=5):\n",
    "    \"\"\"\n",
    "    Función para muestrear de forma coherente de múltiples DataFrames basados en una columna común.\n",
    "\n",
    "    Parámetros:\n",
    "    - dataframes (dict): Un diccionario de DataFrames.\n",
    "    - seed (int): Semilla para la generación de números aleatorios.\n",
    "    - sample_size (int): Número de muestras a seleccionar.\n",
    "\n",
    "    Retorna:\n",
    "    - dict: Un diccionario de DataFrames filtrados.\n",
    "    \"\"\"\n",
    "    # Establecer la semilla aleatoria para reproducibilidad\n",
    "    rnd.seed(seed)\n",
    "\n",
    "    # Determinar los patient_id únicos\n",
    "    unique_patient_ids = set()\n",
    "    for df in dataframes.values():\n",
    "        unique_patient_ids = unique_patient_ids.union(set(df['visit_id']))\n",
    "\n",
    "    # Convertir a lista y muestrear\n",
    "    unique_patient_ids = list(unique_patient_ids)\n",
    "    sampled_patient_ids = rnd.sample(unique_patient_ids, sample_size)\n",
    "\n",
    "    # Filtrar cada DataFrame y retornar un nuevo diccionario\n",
    "    filtered_dfs = {}\n",
    "    for key, df in dataframes.items():\n",
    "        filtered_dfs[key] = df[df['visit_id'].isin(sampled_patient_ids)]\n",
    "\n",
    "    return filtered_dfs\n",
    "\n",
    "# Uso de la función\n",
    "data_loader = DataLoader()\n",
    "dict_of_dfs = {}\n",
    "dict_of_dfs['proteins'], dict_of_dfs['peptides'], dict_of_dfs['clinical'], _ = data_loader.load_train_data()\n",
    "\n",
    "sampled_dfs = sample_dataframes(dict_of_dfs)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(sorted(sampled_dfs['proteins'].patient_id.unique()))\n",
    "print(sorted(sampled_dfs['clinical'].patient_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader()\n",
    "dict_of_dfs = {}\n",
    "dict_of_dfs['proteins'], dict_of_dfs['peptides'], dict_of_dfs['clinical'], _ = data_loader.load_train_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\begin{tabular}{llrrrl}\\n\\\\toprule\\n & Modelo & SMAPE & RMSE & MAE & Dataset \\\\\\\\\\n\\\\midrule\\n0 & updrs_1 & 50.050002 & 5.517838 & 30.446541 & 24 Meses \\\\\\\\\\n1 & updrs_2 & 49.165772 & 5.274010 & 27.815184 & 24 Meses \\\\\\\\\\n2 & updrs_3 & 41.722576 & 12.575038 & 158.131573 & 24 Meses \\\\\\\\\\n3 & updrs_4 & 143.625983 & 3.488770 & 12.171516 & 24 Meses \\\\\\\\\\n4 & Media & 71.141083 & 6.713914 & 57.141204 & 24 Meses \\\\\\\\\\n5 & updrs_1 & 45.203362 & 6.550131 & 42.904215 & 84 Meses \\\\\\\\\\n6 & updrs_2 & 47.470505 & 7.754802 & 60.136950 & 84 Meses \\\\\\\\\\n7 & updrs_3 & 49.618239 & 18.549762 & 344.093659 & 84 Meses \\\\\\\\\\n8 & updrs_4 & 88.098736 & 2.965481 & 8.794080 & 84 Meses \\\\\\\\\\n9 & Media & 57.597711 & 8.955044 & 113.982226 & 84 Meses \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ruta del archivo CSV (reemplázala con tu ruta de archivo real)\n",
    "csv_file_path = 'tu_ruta_aqui.csv'\n",
    "\n",
    "# Leer el archivo CSV desde la ruta especificada\n",
    "df_from_file = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Convertir a formato LaTeX\n",
    "latex_table_from_file = df_grouped_from_file.to_latex(header=True, index=True)\n",
    "\n",
    "print(latex_table_from_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visit_id</th>\n",
       "      <th>visit_month</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>UniProt</th>\n",
       "      <th>NPX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>O00391</td>\n",
       "      <td>11254.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>O00533</td>\n",
       "      <td>732430.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>O00584</td>\n",
       "      <td>39585.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>O14498</td>\n",
       "      <td>41526.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>O14773</td>\n",
       "      <td>31238.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  visit_id  visit_month  patient_id UniProt       NPX\n",
       "0     55_0            0          55  O00391   11254.3\n",
       "1     55_0            0          55  O00533  732430.0\n",
       "2     55_0            0          55  O00584   39585.8\n",
       "3     55_0            0          55  O14498   41526.9\n",
       "4     55_0            0          55  O14773   31238.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_of_dfs['proteins'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 6, 9, 12, 18, 24, 30, 36, 42, 48, 54, 60, 72, 84, 96, 108]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dict_of_dfs['clinical'].visit_month.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visit_id</th>\n",
       "      <th>visit_month</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>UniProt</th>\n",
       "      <th>Peptide</th>\n",
       "      <th>PeptideAbundance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>O00391</td>\n",
       "      <td>NEQEQPLGQWHLS</td>\n",
       "      <td>11254.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>O00533</td>\n",
       "      <td>GNPEPTFSWTK</td>\n",
       "      <td>102060.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>O00533</td>\n",
       "      <td>IEIPSSVQQVPTIIK</td>\n",
       "      <td>174185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>O00533</td>\n",
       "      <td>KPQSAVYSTGSNGILLC(UniMod_4)EAEGEPQPTIK</td>\n",
       "      <td>27278.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>O00533</td>\n",
       "      <td>SMEQNGPGLEYR</td>\n",
       "      <td>30838.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  visit_id  visit_month  patient_id UniProt  \\\n",
       "0     55_0            0          55  O00391   \n",
       "1     55_0            0          55  O00533   \n",
       "2     55_0            0          55  O00533   \n",
       "3     55_0            0          55  O00533   \n",
       "4     55_0            0          55  O00533   \n",
       "\n",
       "                                  Peptide  PeptideAbundance  \n",
       "0                           NEQEQPLGQWHLS           11254.3  \n",
       "1                             GNPEPTFSWTK          102060.0  \n",
       "2                         IEIPSSVQQVPTIIK          174185.0  \n",
       "3  KPQSAVYSTGSNGILLC(UniMod_4)EAEGEPQPTIK           27278.9  \n",
       "4                            SMEQNGPGLEYR           30838.7  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_of_dfs['peptides'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time of Execution: 1.1089999999385327\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.monotonic()\n",
    "\n",
    "data_loader = DataLoader()\n",
    "dict_of_dfs : Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "dict_of_dfs['proteins'], dict_of_dfs['peptides'], dict_of_dfs['clinical'], _ = data_loader.load_train_data()\n",
    "\n",
    "df_after_eda = full_EdaNew(dict_of_dfs)\n",
    "df_after_fe = full_FeatureEngineeringNew(df_after_eda)\n",
    "\n",
    "if_sample = False\n",
    "\n",
    "if if_sample: # For debugging purposes.\n",
    "    df_after_fe = df_after_fe.sample(n=100)\n",
    "\n",
    "train_results = [df_after_fe for i in range(1, 5)] # .drop(columns=[f'updrs_{j}' for j in range(1, 5) if j != i])\n",
    "\n",
    "processed_data = []\n",
    "\n",
    "def split_data(df : pd.DataFrame, target_column : List[str])-> tuple:\n",
    "\n",
    "    test_size = 0.2\n",
    "    val_size = 0.2 / (1 - test_size)  # To maintain the proportion.\n",
    "\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, random_state=42)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "def normalize_and_split_data(df, target_column):\n",
    "\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = split_data(df, target_column)\n",
    "        \n",
    "        # Identificar las columnas que no deben ser escaladas\n",
    "        strings_a_excluir = {'mean', 'median', 'min', 'max', 'std', 'var'}\n",
    "        columnas_sin_cambios = [col for col in df.columns if len(col.split('_')) > 2 if col.split('_')[2] in strings_a_excluir]\n",
    "        columnas_para_escalar = [col for col in df.columns if col not in columnas_sin_cambios and col not in target_column]\n",
    "\n",
    "        preprocesador = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('escalar', StandardScaler(), columnas_para_escalar),\n",
    "                ('sin_cambio', 'passthrough', columnas_sin_cambios)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Aplicar el preprocesador a los conjuntos de entrenamiento, validación y prueba\n",
    "        X_train_norm = pd.DataFrame(preprocesador.fit_transform(X_train), columns=columnas_para_escalar + columnas_sin_cambios)\n",
    "        X_val_norm = pd.DataFrame(preprocesador.transform(X_val), columns=columnas_para_escalar + columnas_sin_cambios)\n",
    "        X_test_norm = pd.DataFrame(preprocesador.transform(X_test), columns=columnas_para_escalar + columnas_sin_cambios)\n",
    "        \n",
    "        return X_train_norm, X_val_norm, X_test_norm, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "for df, target_column in zip(train_results, [f'updrs_{i+1}' for i in range(len(train_results))]):\n",
    "    processed_data.append(normalize_and_split_data(df, target_column))\n",
    "\n",
    "end_time = time.monotonic()\n",
    "\n",
    "print(f\"\\nTime of Execution: {end_time - start_time}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of utils failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\monfm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\monfm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\monfm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py\", line 168, in reload\n",
      "    raise ModuleNotFoundError(f\"spec not found for the module {name!r}\", name=name)\n",
      "ModuleNotFoundError: spec not found for the module 'utils'\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "class EdaNew:\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_clinical_by_month(clinical_df : pd.DataFrame, month_to_filter : int = 24, updrs_cols : List[str] = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Filter the clinical data by month and select the columns of interest.\n",
    "        \"\"\"\n",
    "\n",
    "        clinical_df_filtered_by_month = clinical_df[clinical_df.visit_month == month_to_filter]\n",
    "\n",
    "        return clinical_df_filtered_by_month\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter_proteins_by_month(proteins_df: pd.DataFrame, month_to_filter : int = 24) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Filter the proteins data by month and select the columns of interest.\n",
    "        \"\"\"\n",
    "\n",
    "        proteins_df_filtered_by_month = proteins_df[proteins_df.visit_month == month_to_filter]\n",
    "\n",
    "        return proteins_df_filtered_by_month\n",
    "    \n",
    "    def filter_peptides_by_month(peptides_df: pd.DataFrame, month_to_filter : int = 24) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Filter the peptides data by month and select the columns of interest.\n",
    "        \"\"\"\n",
    "\n",
    "        peptides_df_filtered_by_month = peptides_df[peptides_df.visit_month == month_to_filter]\n",
    "\n",
    "        return peptides_df_filtered_by_month\n",
    "\n",
    "    @staticmethod\n",
    "    def analyze_and_visualize_duplicates(data, title :str = 'DataFrame', verbose : bool = False): # TODO analizar y visualizar duplicados.\n",
    "        \"\"\"\n",
    "        Función para analizar y opcionalmente visualizar y eliminar datos duplicados en un DataFrame basándose en columnas específicas.\n",
    "\n",
    "        Args:\n",
    "        data (pd.DataFrame): DataFrame a analizar.\n",
    "        title (str): Título para la visualización.\n",
    "        index_cols (list): Lista de columnas para identificar duplicados.\n",
    "        verbose (bool): Si es True, imprime información y muestra gráficos.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: DataFrame con duplicados eliminados.\n",
    "        \"\"\"\n",
    "\n",
    "        duplicate_rows = data.duplicated(keep=False)\n",
    "        num_duplicate_rows = duplicate_rows.sum()\n",
    "        proportion_duplicates = num_duplicate_rows / len(data) * 100\n",
    "\n",
    "        # Eliminar duplicados\n",
    "        data_filtered = data.drop_duplicates()\n",
    "\n",
    "        if verbose:\n",
    "            # Gráfico\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            sns.countplot(x=duplicate_rows)\n",
    "            plt.title(f'Duplicate Counts in {title}')\n",
    "            plt.ylabel('Count')\n",
    "            plt.xlabel('Is Duplicate')\n",
    "\n",
    "            # Mostrar información de duplicados\n",
    "            print(f'{title} - Proportion of Duplicates: {proportion_duplicates:.2f}%')\n",
    "            print(f'{title} - Number of Duplicate Rows: {num_duplicate_rows}')\n",
    "            num_rows_removed = len(data) - len(data_filtered)\n",
    "            print(f'{title} - Number of Rows Removed: {num_rows_removed}')\n",
    "\n",
    "        return data_filtered\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_and_remove_null_values(df, groupby_column='Proteins', verbose=False): #TODO calcular y eliminar valores nulos.\n",
    "        \"\"\"\n",
    "        Función para calcular y opcionalmente visualizar y eliminar filas con valores nulos en un DataFrame.\n",
    "\n",
    "        Args:\n",
    "        df (pd.DataFrame): DataFrame a analizar.\n",
    "        groupby_column (str): Nombre de la columna para agrupar los resultados.\n",
    "        verbose (bool): Si es True, imprime información y muestra gráficos.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: DataFrame con filas nulas eliminadas.\n",
    "        \"\"\"\n",
    "        # Crear una copia del DataFrame para no modificar el original\n",
    "        temp_df = df.copy()\n",
    "\n",
    "        # Calcular la cantidad de valores nulos en cada fila\n",
    "        temp_df[\"null_count\"] = temp_df.isnull().sum(axis=1)\n",
    "\n",
    "        # Filtrar las filas que tienen al menos un valor nulo\n",
    "        df_with_nulls = temp_df[temp_df[\"null_count\"] > 0]\n",
    "\n",
    "        # Información sobre los valores nulos\n",
    "        num_rows_with_nulls = len(df_with_nulls)\n",
    "        total_rows = len(df)\n",
    "        proportion_nulls = num_rows_with_nulls / total_rows * 100\n",
    "\n",
    "        # Eliminar filas con valores nulos\n",
    "        data_filtered = temp_df[temp_df[\"null_count\"] == 0]\n",
    "\n",
    "        if verbose:\n",
    "            # Gráfico\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            sns.histplot(temp_df['null_count'], bins=range(1, temp_df['null_count'].max() + 1), kde=False)\n",
    "            plt.title(f'Null Value Counts in {groupby_column}')\n",
    "            plt.ylabel('Count')\n",
    "            plt.xlabel('Number of Null Values')\n",
    "\n",
    "            # Mostrar información de valores nulos\n",
    "            print(f'{groupby_column} - Proportion of Rows with Nulls: {proportion_nulls:.2f}%')\n",
    "            print(f'{groupby_column} - Number of Rows with Nulls: {num_rows_with_nulls}')\n",
    "            num_rows_removed = total_rows - len(data_filtered)\n",
    "            print(f'{groupby_column} - Number of Rows Removed: {num_rows_removed}')\n",
    "\n",
    "        return data_filtered\n",
    "    \n",
    "    @staticmethod #TODO\n",
    "    def remove_outliers_iqr(df, columns = ['NPX', 'PeptideAbundance'], iqr_factor=1.5):\n",
    "\n",
    "        \"\"\"Elimina los outliers basados en el Rango Intercuartílico (IQR).\"\"\"\n",
    "\n",
    "        for column in columns:\n",
    "            Q1 = df[column].quantile(0.25)\n",
    "            Q3 = df[column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "\n",
    "            rango_inferior = Q1 - iqr_factor * IQR\n",
    "            rango_superior = Q3 + iqr_factor * IQR\n",
    "\n",
    "            df = df[(df[column] >= rango_inferior) & (df[column] <= rango_superior)]\n",
    "        return df\n",
    "    \n",
    "    @staticmethod #TODO\n",
    "    def add_log_columns(df, columns=['NPX', 'PeptideAbundance']):\n",
    "\n",
    "        \"\"\"Añade columnas logarítmicas para las columnas especificadas.\"\"\"\n",
    "\n",
    "        for column in columns:\n",
    "            df[f'{column}_log'] = np.log(df[column])\n",
    "        return df\n",
    "\n",
    "    @staticmethod #TODO\n",
    "    def remove_outliers_std(df, columns=['NPX', 'PeptideAbundance'], std_factor=3):\n",
    "        \"\"\"\n",
    "        Elimina los outliers basados en la desviación estándar.\n",
    "        \"\"\"\n",
    "            \n",
    "        for column in columns:\n",
    "            mean = df[column].mean()\n",
    "            std = df[column].std()\n",
    "\n",
    "            df = df[(df[column] >= mean - std_factor * std) & (df[column] <= mean + std_factor * std)]\n",
    "        return df\n",
    "    \n",
    "    @staticmethod #TODO\n",
    "    def drop_upd23b_clinical_state_on_medication(df):\n",
    "        df_transformed = df.drop(['upd23b_clinical_state_on_medication'], axis=1)\n",
    "        return df_transformed\n",
    "\n",
    "    @staticmethod #TODO\n",
    "    def drop_group_key(df):\n",
    "        df_transformed = df.drop(['group_key'], axis=1)\n",
    "        return df_transformed\n",
    "    \n",
    "    @staticmethod #TODO\n",
    "    def drop_null_count(df):\n",
    "        df_transformed = df.drop(['null_count'], axis=1)\n",
    "        return df_transformed\n",
    "    \n",
    "    @staticmethod #TODO\n",
    "    def drop_visit_id_and_visit_month(df):\n",
    "        df_transformed = df.drop(['visit_id', 'visit_month'], axis=1).reset_index(drop=True)\n",
    "        return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_results[0].shape[0] == 47208, \"The execution dont respect the dimension of the data during the process\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
